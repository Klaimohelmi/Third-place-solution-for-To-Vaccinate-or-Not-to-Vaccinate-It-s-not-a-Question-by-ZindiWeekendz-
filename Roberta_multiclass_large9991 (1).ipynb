{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta_multiclass_large9991.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsKEvcIUSqyn",
        "colab_type": "code",
        "outputId": "ef1f6690-56a9-4031-9233-9ef01450cec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Aj4JKHNHcZ",
        "colab_type": "code",
        "outputId": "44e7c05c-fcba-4805-ff19-30566a960807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/drive/My Drive/Train(8).csv', usecols=[\"safe_text\", \"label\"])\n",
        "test_df=pd.read_csv('/content/drive/My Drive/Test(6).csv', usecols=[\"safe_text\"])\n",
        "feat_cols = \"safe_text\"\n",
        "label_cols = \"label\"\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>safe_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I mean if they immunize my kid with something ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           safe_text  label\n",
              "0  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0\n",
              "1  I'm 100% thinking of devoting my career to pro...    1.0\n",
              "2  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0\n",
              "3  I mean if they immunize my kid with something ...   -1.0\n",
              "4  Thanks to <user> Catch me performing at La Nui...    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llYLC46J6XkN",
        "colab_type": "code",
        "outputId": "b8b5c9d7-0957-4b32-dcb3-f27598db445a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>safe_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; ... &amp;amp; 4 a vaccine given 2 he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Students starting school without whooping coug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I'm kinda over every ep of &lt;user&gt; being \"rippe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many innocent children die for lack of vac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CDC eyeing bird flu vaccine for humans, though...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           safe_text\n",
              "0  <user> <user> ... &amp; 4 a vaccine given 2 he...\n",
              "1  Students starting school without whooping coug...\n",
              "2  I'm kinda over every ep of <user> being \"rippe...\n",
              "3  How many innocent children die for lack of vac...\n",
              "4  CDC eyeing bird flu vaccine for humans, though..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kqvaCZUP_oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df[feat_cols] = test_df[feat_cols].astype(str)\n",
        "train[feat_cols] = train[feat_cols].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc1GShXpS1FM",
        "colab_type": "code",
        "outputId": "ecb9e43b-d4e6-439f-cbb2-6e32544691b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train['label'].isna().sum().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-MWvOLaS-c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=train.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwX1oQc_TJnW",
        "colab_type": "code",
        "outputId": "7fb711cc-4af6-48ec-cd41-18c010dbefc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>safe_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I mean if they immunize my kid with something ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           safe_text  label\n",
              "0  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0\n",
              "1  I'm 100% thinking of devoting my career to pro...    1.0\n",
              "2  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0\n",
              "3  I mean if they immunize my kid with something ...   -1.0\n",
              "4  Thanks to <user> Catch me performing at La Nui...    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyNj07OzPPj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df=test_df.fillna('nan')\n",
        "#train=train.fillna('nan')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMgbom46wt4",
        "colab_type": "code",
        "outputId": "ae69cf99-a719-4b8f-a308-48aed97173db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape,test_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 2), (5177, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y_dKnH6Gy6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    #Remove emojis and special chars\n",
        "    clean=text\n",
        "    #reg = re.compile('\\\\.+?(?=\\B|$)')\n",
        "    #clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n",
        "    #reg = re.compile('\\x89Û_')\n",
        "    #clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
        "    reg = re.compile('\\&amp')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n",
        "    reg = re.compile('\\\\n')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
        "\n",
        "    #Remove hashtag symbol (#)\n",
        "    #clean = clean.apply(lambda r: r.replace('#', ''))\n",
        "\n",
        "    #Remove user names\n",
        "    reg = re.compile('@[a-zA-Z0-9\\_]+')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n",
        "\n",
        "    #Remove URLs\n",
        "    reg = re.compile('https?\\S+(?=\\s|$)')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='www'))\n",
        "\n",
        "    #Lowercase\n",
        "    #clean = clean.apply(lambda r: r.lower())\n",
        "    return clean\n",
        "train['safe_text'] = clean_text(train['safe_text'])\n",
        "test_df['safe_text'] = clean_text(test_df['safe_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BagzGWT2HFB9",
        "colab_type": "code",
        "outputId": "16062bb0-2aed-46f3-c17e-8dd6f1e12b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>safe_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I mean if they immunize my kid with something ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           safe_text  label\n",
              "0  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0\n",
              "1  I'm 100% thinking of devoting my career to pro...    1.0\n",
              "2  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0\n",
              "3  I mean if they immunize my kid with something ...   -1.0\n",
              "4  Thanks to <user> Catch me performing at La Nui...    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdw_FQRoVGsJ",
        "colab_type": "code",
        "outputId": "7d6d8758-c919-4779-c852-f0025a682e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "! pip install pytorch-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 26.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.4.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 26.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 34.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 40.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 27.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 20.3MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 22.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 20.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 18.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 19.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 15.6MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 15.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 15.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.12.43)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.38.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.15.43)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=669ab56426e334ea2d8635e41184f5ec45746cb6d8bb8c125e09c093c08df211\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.41 sentencepiece-0.1.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4CPH9THVTOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import *\n",
        "from fastai.metrics import *\n",
        "from pytorch_transformers import RobertaTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5r_eEXiVXFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a config object to store task specific information\n",
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "        \n",
        "config = Config(\n",
        "    testing=False,\n",
        "    seed = 2019,\n",
        "    roberta_model_name='roberta-large', # can also be exchnaged with roberta-large \n",
        "    max_lr=1e-5,\n",
        "    epochs=2,\n",
        "    use_fp16=False,\n",
        "    bs=4, \n",
        "    max_seq_len=96, \n",
        "    num_labels = 3,\n",
        "    hidden_dropout_prob=.05,\n",
        "    hidden_size=1024, # 1024 for roberta-large\n",
        "    start_tok = \"<s>\",\n",
        "    end_tok = \"</s>\",\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbrdKZA8GZUU",
        "colab_type": "code",
        "outputId": "0b54ca1c-c74b-4357-f340-330a4f0c0398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "safe_text     object\n",
              "label        float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHWyZmnRVrpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastAiRobertaTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
        "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len \n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self \n",
        "    def tokenizer(self, t:str) -> List[str]: \n",
        "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
        "        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWhfGesjVyho",
        "colab_type": "code",
        "outputId": "e492d67d-275f-4224-9e97-be909013ed02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# create fastai tokenizer for roberta\n",
        "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "\n",
        "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n",
        "                             pre_rules=[], post_rules=[])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 898823/898823 [00:00<00:00, 2270402.14B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 1813552.65B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmu1WUEWV0-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create fastai vocabulary for roberta\n",
        "path = Path()\n",
        "roberta_tok.save_vocabulary(path)\n",
        "\n",
        "with open('vocab.json', 'r') as f:\n",
        "    roberta_vocab_dict = json.load(f)\n",
        "    \n",
        "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIS62S1RV4LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up pre-processors\n",
        "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
        "    def __init__(self, tokenizer):\n",
        "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n",
        "\n",
        "\n",
        "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    \"\"\"\n",
        "    Constructing preprocessors for Roberta\n",
        "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
        "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
        "    \"\"\"\n",
        "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mO9tdl5WfXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a Roberta specific DataBunch class\n",
        "class RobertaDataBunch(TextDataBunch):\n",
        "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
        "    @classmethod\n",
        "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
        "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
        "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
        "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
        "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
        "        val_bs = ifnone(val_bs, bs)\n",
        "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
        "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
        "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
        "        dataloaders = [train_dl]\n",
        "        for ds in datasets[1:]:\n",
        "            lengths = [len(t) for t in ds.x.items]\n",
        "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
        "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
        "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAkVM0u7Wm1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RobertaTextList(TextList):\n",
        "    _bunch = RobertaDataBunch\n",
        "    _label_cls = TextList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf-DIs-gDHEi",
        "colab_type": "code",
        "outputId": "ad7e4861-9a3a-4ac0-83b0-4016f68ef6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5177, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIa4qnnFBags",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub=pd.read_csv('/content/drive/My Drive/SampleSubmission(18).csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk8deCt49tZj",
        "colab_type": "code",
        "outputId": "aa201576-c5e8-43fb-c8ef-40765054799e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=999) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=FloatList) \\\n",
        "    .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor)) \\\n",
        "    .databunch(bs=4, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=1):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "roberta_model = CustomRobertaModel()\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[rmse])\n",
        "\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(3, max_lr=config.max_lr)\n",
        "\n",
        "\n",
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "test_preds,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "sub['label']=test_preds\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p<-1:\n",
        "    sub['label'][e]=-1\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p>1:\n",
        "    sub['label'][e]=1\n",
        "\n",
        "sub.to_csv('sentimentsub1.csv',index=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 482/482 [00:00<00:00, 421265.79B/s]\n",
            "100%|██████████| 1425941629/1425941629 [00:32<00:00, 44488165.13B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>root_mean_squared_error</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.344478</td>\n",
              "      <td>0.310786</td>\n",
              "      <td>0.491188</td>\n",
              "      <td>05:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.224179</td>\n",
              "      <td>0.230973</td>\n",
              "      <td>0.397154</td>\n",
              "      <td>05:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.199767</td>\n",
              "      <td>0.213947</td>\n",
              "      <td>0.373155</td>\n",
              "      <td>05:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWi376DQW7yx",
        "colab_type": "code",
        "outputId": "14064f67-0d5e-4f0f-91a4-74717ee36d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=30000) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=FloatList) \\\n",
        "    .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor)) \\\n",
        "    .databunch(bs=4, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=1):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "roberta_model = CustomRobertaModel()\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[rmse])\n",
        "\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(3, max_lr=config.max_lr)\n",
        "\n",
        "\n",
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "test_preds2,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "sub['label']=test_preds2\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p<-1:\n",
        "    sub['label'][e]=-1\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p>1:\n",
        "    sub['label'][e]=1\n",
        "\n",
        "sub.to_csv('sentimentsub2.csv',index=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>root_mean_squared_error</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.297060</td>\n",
              "      <td>0.360069</td>\n",
              "      <td>0.536846</td>\n",
              "      <td>05:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.225564</td>\n",
              "      <td>0.216555</td>\n",
              "      <td>0.390990</td>\n",
              "      <td>05:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.149344</td>\n",
              "      <td>0.222103</td>\n",
              "      <td>0.376802</td>\n",
              "      <td>05:30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lNoOj_qS3JX",
        "colab_type": "code",
        "outputId": "cc44e61c-4152-46aa-e9e4-8b6be613cd23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=42) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=FloatList) \\\n",
        "    .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor)) \\\n",
        "    .databunch(bs=4, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=1):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "roberta_model = CustomRobertaModel()\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[rmse])\n",
        "\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(3, max_lr=config.max_lr)\n",
        "\n",
        "\n",
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "test_preds3,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "sub['label']=test_preds3\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p<-1:\n",
        "    sub['label'][e]=-1\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p>1:\n",
        "    sub['label'][e]=1\n",
        "\n",
        "sub.to_csv('sentimentsub3.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>root_mean_squared_error</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.286233</td>\n",
              "      <td>0.320767</td>\n",
              "      <td>0.499708</td>\n",
              "      <td>05:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.200946</td>\n",
              "      <td>0.324632</td>\n",
              "      <td>0.475297</td>\n",
              "      <td>05:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.205122</td>\n",
              "      <td>0.227666</td>\n",
              "      <td>0.377857</td>\n",
              "      <td>05:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhJZAXwdUao6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub1=pd.read_csv('/content/sentimentsub1.csv')\n",
        "sub2=pd.read_csv('/content/sentimentsub2.csv')\n",
        "sub3=pd.read_csv('/content/sentimentsub3.csv')\n",
        "sub1['label']=0.35*sub1['label']+0.3*sub2['label']+0.35*sub3['label']\n",
        "sub1.to_csv('sentimentsub_ens.csv',index=False)#0.5040"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY_UxPhlOOzj",
        "colab_type": "code",
        "outputId": "b232a3d0-755b-43d5-8828-69b9a7c17305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=2020) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=FloatList) \\\n",
        "    .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor)) \\\n",
        "    .databunch(bs=4, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=1):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "roberta_model = CustomRobertaModel()\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[rmse])\n",
        "\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(3, max_lr=config.max_lr)\n",
        "\n",
        "\n",
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "test_preds4,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "sub['label']=test_preds4\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p<-1:\n",
        "    sub['label'][e]=-1\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p>1:\n",
        "    sub['label'][e]=1\n",
        "\n",
        "sub.to_csv('sentimentsub4.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>root_mean_squared_error</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.310462</td>\n",
              "      <td>0.310225</td>\n",
              "      <td>0.495539</td>\n",
              "      <td>05:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.220305</td>\n",
              "      <td>0.241209</td>\n",
              "      <td>0.436551</td>\n",
              "      <td>05:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.167402</td>\n",
              "      <td>0.239809</td>\n",
              "      <td>0.396257</td>\n",
              "      <td>05:32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2oCJlMqOqiE",
        "colab_type": "code",
        "outputId": "62c74c03-531b-46bf-b098-1ffa829654ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=80000) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=FloatList) \\\n",
        "    .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor)) \\\n",
        "    .databunch(bs=4, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=1):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "roberta_model = CustomRobertaModel()\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[rmse])\n",
        "\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(3, max_lr=config.max_lr)\n",
        "\n",
        "\n",
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "test_preds5,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "sub['label']=test_preds5\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p<-1:\n",
        "    sub['label'][e]=-1\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p>1:\n",
        "    sub['label'][e]=1\n",
        "\n",
        "sub.to_csv('sentimentsub5.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>root_mean_squared_error</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.285385</td>\n",
              "      <td>0.278302</td>\n",
              "      <td>0.472966</td>\n",
              "      <td>05:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.253556</td>\n",
              "      <td>0.224209</td>\n",
              "      <td>0.391467</td>\n",
              "      <td>05:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.153945</td>\n",
              "      <td>0.230735</td>\n",
              "      <td>0.389740</td>\n",
              "      <td>05:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY3HOGsAO0MY",
        "colab_type": "code",
        "outputId": "88427d04-a7ad-4b29-9941-1b8575844896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "    .split_by_rand_pct(seed=666) \\\n",
        "    .label_from_df(cols=label_cols,label_cls=FloatList) \\\n",
        "    .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor)) \\\n",
        "    .databunch(bs=4, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=1):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "roberta_model = CustomRobertaModel()\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[rmse])\n",
        "\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(3, max_lr=config.max_lr)\n",
        "\n",
        "\n",
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "test_preds6,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "sub['label']=test_preds6\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p<-1:\n",
        "    sub['label'][e]=-1\n",
        "\n",
        "for e,p in enumerate(sub['label']):\n",
        "  if p>1:\n",
        "    sub['label'][e]=1\n",
        "\n",
        "sub.to_csv('sentimentsub6.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>root_mean_squared_error</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.272258</td>\n",
              "      <td>0.400669</td>\n",
              "      <td>0.544537</td>\n",
              "      <td>05:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.236850</td>\n",
              "      <td>0.231394</td>\n",
              "      <td>0.402734</td>\n",
              "      <td>05:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.109465</td>\n",
              "      <td>0.226802</td>\n",
              "      <td>0.384359</td>\n",
              "      <td>05:27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr3bFZO6O7ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub4=pd.read_csv('/content/sentimentsub4.csv')\n",
        "sub5=pd.read_csv('/content/sentimentsub5.csv')\n",
        "sub6=pd.read_csv('/content/sentimentsub6.csv')\n",
        "sub4['label']=0.3*sub4['label']+0.3*sub5['label']+0.4*sub6['label']\n",
        "sub4.to_csv('sentimentsub_ens1.csv',index=False)#0.50120"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWPFjOZKXUtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub11=pd.read_csv('/content/sentimentsub_ens.csv')\n",
        "sub22=pd.read_csv('/content/sentimentsub_ens1.csv')\n",
        "sub11['label']=0.5*sub11['label']+0.5*sub22['label']\n",
        "sub11.to_csv('final_sub.csv',index=False)#0.49818"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmvDX4v6axhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lstm approach"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHarJxgnZN1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "681a7521-118b-41a3-8818-81deac434668"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# Setup\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "import pandas as pd\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.optimizers import RMSprop,SGD\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "df=pd.read_csv('/content/Train (10).csv')\n",
        "\n",
        "df = df.loc[df['agreement'] > 0.34]\n",
        "df=df.dropna()\n",
        "Y = df.label\n",
        "\n",
        "# Print initial shape\n",
        "print(Y.shape)\n",
        "X=df.safe_text\n",
        "\n",
        "\n",
        "max_len = 200\n",
        "# Build the dictionary of indexes\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(X)\n",
        "# Change texts into sequence of indexes\n",
        "sequences = tok.texts_to_sequences(X)\n",
        "# Pad the sequences\n",
        "sequences_matrix_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "\n",
        "\n",
        "test=pd.read_csv('/content/Test (5).csv')\n",
        "test=test.safe_text\n",
        "test=test.fillna(method='ffill')\n",
        "#same preprocessing for test \n",
        "\n",
        "tok.fit_on_texts(test)\n",
        "sequences = tok.texts_to_sequences(test)\n",
        "sequences_matrix_test = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "(9760,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6kJJS2Z1tV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "24cb9e88-93be-4086-cee0-0c92b22f07fb"
      },
      "source": [
        "#choosing vocabulary for embedding layer \n",
        "max_words=2000000\n",
        "def RNN():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = (Conv1D(filters=64,\n",
        "                 kernel_size=4,\n",
        "                 padding='valid',\n",
        "                 activation='sigmoid',\n",
        "                 strides=1))(layer)\n",
        "    layer = (MaxPooling1D(pool_size=2))(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = LSTM(64)(layer)\n",
        "    \n",
        "    \n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('tanh')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model \n",
        "model = RNN()\n",
        "model.summary()\n",
        "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 200, 50)           100000000 \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 200, 50)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 197, 64)           12864     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 98, 64)            0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 98, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 100,062,785\n",
            "Trainable params: 100,062,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaecxOtmaCXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "5b318491-8427-43f2-9d50-a5768c8cf669"
      },
      "source": [
        "model.fit(sequences_matrix_train,Y,batch_size=128,epochs=8,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7808 samples, validate on 1952 samples\n",
            "Epoch 1/8\n",
            "7808/7808 [==============================] - 13s 2ms/step - loss: 0.4033 - accuracy: 0.4982 - val_loss: 0.3759 - val_accuracy: 0.5020\n",
            "Epoch 2/8\n",
            "7808/7808 [==============================] - 12s 2ms/step - loss: 0.3939 - accuracy: 0.5022 - val_loss: 0.3754 - val_accuracy: 0.5020\n",
            "Epoch 3/8\n",
            "7808/7808 [==============================] - 12s 2ms/step - loss: 0.3871 - accuracy: 0.5063 - val_loss: 0.3565 - val_accuracy: 0.5666\n",
            "Epoch 4/8\n",
            "7808/7808 [==============================] - 12s 2ms/step - loss: 0.3399 - accuracy: 0.6119 - val_loss: 0.3136 - val_accuracy: 0.6675\n",
            "Epoch 5/8\n",
            "7808/7808 [==============================] - 12s 2ms/step - loss: 0.2881 - accuracy: 0.7011 - val_loss: 0.3107 - val_accuracy: 0.6685\n",
            "Epoch 6/8\n",
            "7808/7808 [==============================] - 12s 2ms/step - loss: 0.2513 - accuracy: 0.7432 - val_loss: 0.3090 - val_accuracy: 0.6767\n",
            "Epoch 7/8\n",
            "7808/7808 [==============================] - 12s 2ms/step - loss: 0.2252 - accuracy: 0.7681 - val_loss: 0.3214 - val_accuracy: 0.6803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7efdcfad34a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhwhF4_QbZzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p=model.predict(sequences_matrix_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTWW5RADbxpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "952e3be1-a37c-47a5-8996-cf52f69228d1"
      },
      "source": [
        "p"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.10419193],\n",
              "       [ 0.2455794 ],\n",
              "       [ 0.03512662],\n",
              "       ...,\n",
              "       [ 0.7977812 ],\n",
              "       [ 0.00842187],\n",
              "       [ 0.42403623]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjOXRqjShJXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ea432e0a-6f2b-43e9-8c44-e8135b87d06d"
      },
      "source": [
        "#choosing vocabulary for embedding layer \n",
        "max_words=2000000\n",
        "def RNN():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    \n",
        "\n",
        "    layer = LSTM(64)(layer)\n",
        "    \n",
        "    \n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('tanh')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model \n",
        "model = RNN()\n",
        "model.summary()\n",
        "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 200, 50)           100000000 \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 64)                29440     \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 100,046,337\n",
            "Trainable params: 100,046,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42loclXkhqX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "b23101fb-3cdd-4477-ed96-8eccd5a09618"
      },
      "source": [
        "model.fit(sequences_matrix_train,Y,batch_size=128,epochs=2,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7808 samples, validate on 1952 samples\n",
            "Epoch 1/2\n",
            "7808/7808 [==============================] - 22s 3ms/step - loss: 0.3722 - accuracy: 0.5499 - val_loss: 0.3008 - val_accuracy: 0.6716\n",
            "Epoch 2/2\n",
            "7808/7808 [==============================] - 21s 3ms/step - loss: 0.2479 - accuracy: 0.7367 - val_loss: 0.2958 - val_accuracy: 0.6875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7efdcf6e9390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXBoX40xhEZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q=model.predict(sequences_matrix_test)\n",
        "a=q*2/3+p*1/3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yta0VMTeB6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub=pd.read_csv('/content/SampleSubmission (11).csv')\n",
        "sub['label']=a\n",
        "final=pd.read_csv('/content/final_sub.csv')\n",
        "final['label']=final['label']*0.85+sub['label']*0.15\n",
        "final.to_csv('se.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS0G6ORoiAw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#0.487578702571676"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbcraCUjikve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}